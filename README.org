#+TITLE: Code used for PhD project
#+DATE: 2020-07-25
#+OPTIONS: toc:nil author:nil title:nil date:nil num:nil ^:{} \n:1 todo:nil
#+PROPERTY: header-args :eval never-export
#+LATEX_HEADER: \usepackage[margin=1.0in]{geometry}
#+LATEX_HEADER: \hypersetup{colorlinks=true,citecolor=black,linkcolor=black,urlcolor=blue,linkbordercolor=blue,pdfborderstyle={/S/U/W 1}}
#+LATEX_HEADER: \usepackage[round]{natbib}
#+LATEX_HEADER: \renewcommand{\bibsection}
#+ARCHIVE: daily_archive.org::datetree/* From master todo

Analysis and utility code used for thesis project. 
* Staying Organized

Preventing spaghetti code and remembering how things are done is of utmost importance. It is amazing how quickly I forget how some analysis was done, or what files were included. 
We use scripting languages to perform serious data analysis, and we have the benefit of reproducible research (ability to re-run the analysis and get the same result). However, this benefit is only automatic for simple scripts. Often I am re-running an analysis with different parameters, or different input files, and this often leads to copying the main script file with new hard-coded inputs, or commenting and re-assigning global input variables. Which analysis led to which output? Which is most recent? What exactly were the input files? 
I've only been working on this project for a month and I am already facing this issue. I am trying to stay consistent, so I am adopting the following method going forward:

1. Analysis is developed using *_scratch files.
2. Once I have it worked out, the library functions go in a file under =patch_clamp= or =lfp= and I write a batch function that lives in =scripts/=.
3. The batch file will take command line inputs. So a path list, and any parameters.
4. Batch files will then be run with shell scripts. 

The idea is that batches can be re-run with different parameters using a shell script, which documents exactly what was done, what the outputs were, and allows it to be re-run.

** Creating new shell scripts

Scripts will be run from =ephys/=, and this becomes the working directory. Ensure you are in the =patch= virtualenv before you run a script. Write the shell script, then run =chmod +x <script-name>= to make executable. 

Scripts can then be run via =shell/<script-name.sh>=


* LFP
- analysis routines for LFP extracellular recording analysis.
** script design
** DONE add IO file organization
   CLOSED: [2020-06-27 Sat 06:58]
** DONE write analysis which will save images of each experiment
   CLOSED: [2020-06-27 Sat 06:58]
** DONE save data from each good experiment?
   CLOSED: [2020-06-27 Sat 06:58]
Manual writing an =exclude.json=

* patch_clamp
** spikes analysis
- /I am re-running this analysis using the methods described under [[Staying Organized]]/
- Most recent run was with git hash: 1b61c2d
- Database path is =/Users/nick/Dropbox/lab_notebook/projects_and_data/mnc/analysis_and_data/patch_clamp/data/summary_data/spiking_201912-202001/patch_data_batch.db=
- =update_metadata_db.sh= adds the metadata to the database
- =spikes.sh= will be the main driver adding the peaks to the database
*TODO* ADD a new script adding the peak times directly to a database table. this will mean modifying =serialize_cc01.py= to add directly to the database. 
*TODO* Check it in R. 

*** details for steps spikes analysis

 - =count_steps.py= iterates through all paths and all sweeps, recording all the peaks in the database. 
 - This script can be run from the =patch= virtual environment, from the =ephys= directory using:
   =python patch_clamp/count_steps.py=
   - Careful, the query string is specific to the database table!
 - For analysis, we will pull those values out of the database and write them to =json= via =serialize_cc01.py=. From the =ephys= directory use:
   =python patch_clamp/serialize_cc01.py=
   - Careful, this is a rough file with vars you need to change throughout.
   - Ensure the query string constant is correct /and/ ensure the file path for write out is correct.
   - The current set will end with =_higher_threhsold.json=.
 - =membrane_potential.py= measures the mean membrane potential for the first ~0.5s of the file to get a baseline for possible exclusion of files later. 

** R analysis steps for spikes
*** DONE map sweeps to current steps
    CLOSED: [2020-10-14 Wed 14:24]
*** DONE add to database table
    CLOSED: [2020-10-14 Wed 16:47]
*** In-progress compare half max current and max current step freq
*** TODO glm 
=spike ~ current+side*treatment= 
ensure that the side is L/R for all
*** ISI calculations. need to test this

* utility
Miscellaneous scripts used for analysis or organization tasks
** Table of contents
